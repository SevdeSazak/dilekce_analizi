{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZSmmnL3exzRL",
    "outputId": "a210cbfd-e770-45c1-9977-d4f8b7bc3580"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.6)\n",
      "Requirement already satisfied: pdfminer.six==20250327 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20250327)\n",
      "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.2.1)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.1)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (3.4.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (43.0.3)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.22)\n",
      "Requirement already satisfied: stanza in /usr/local/lib/python3.11/dist-packages (1.10.1)\n",
      "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (from stanza) (2.14.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from stanza) (2.0.2)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.11/dist-packages (from stanza) (5.29.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from stanza) (2.32.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from stanza) (3.5)\n",
      "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from stanza) (2.6.0+cu124)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from stanza) (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (4.14.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.3.0->stanza) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.3.0->stanza) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber\n",
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1BAuvuGyM7C"
   },
   "source": [
    "Kütüphaneler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "id": "8P6riIZ9yFeZ"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import re\n",
    "import pdfplumber\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5WncIUUyN1E"
   },
   "source": [
    "PDF Çevirme İşlemi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-RCaUnmFyHoy",
    "outputId": "2712003e-f3be-4fe6-ea3c-716d3a22189e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
      "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sayın Valilik Makamına, Geçtiğimiz ay okul önünde yaşanan trafik kazasından sonra bölgede hâlâ herhangi bir trafik ışığı yerleştirilmedi. Çocuklarımızın can güvenliği için acilen önlem alınması gerekiyor. Konunun dikkate alınmasını arz ederim. Zehra Altın, Merkez / Tokat\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\n",
    "with pdfplumber.open(\"/content/drive/MyDrive/case4.pdf\") as pdf:\n",
    "    for page in pdf.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text.replace('\\n', ' ')\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbMHF6QayrSe"
   },
   "source": [
    "\n",
    "Modeller\n",
    "1.  BERTurk (savasy): Bu model kişi adı soyadı, adres, tarih ve kurum gibi bilgi çıkarımı için kullanılmıştır.\n",
    "2.  Stanza (Stanford): Lemmatizationişlemi için kullanılmıştır.\n",
    "3.  Zero-Shot-Classification: Olay konusu ve talep için kullanılmıştır.\n",
    "4. Text-Classification: Dilin Resmiyetininde argo kelimeleri tespit etmek için kullanılmıştır.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708,
     "referenced_widgets": [
      "0b457ecb846346e4a59898d251c4e6f3",
      "8aecc15da79b4dc2b3d251425b84a960",
      "39ce490cc05c4ca7af535f994b72bf6d",
      "fd69b24091494353b5b459c89e9b8697",
      "f790a5d56c7d445b9b7e615b76f197a5",
      "46249a5ceb0e4410b165d862f5bd975c",
      "0635621c8f7e403f91e2441603a5fe58",
      "5094a8b4ae2f4487b5276380ab165cc2",
      "5fde35e24ae94749adeeafb5a590ff2b",
      "f766deebffb24272b6ce6f868cc325f8",
      "0df18b79f2594333aa803a8705f7701d",
      "749d2545fa1a4a37891bfef7ec1a1469",
      "6d33d9e1b5c94cb1b0955ed1e132b36c",
      "1ab0abe12ac84002ab1b8e276ad24182",
      "7c72099df3224adfb612a59f02318dec",
      "072135cdbdf746bb8878007bd8935acd",
      "7f487ec703854b42b04d5a6530cb4c98",
      "1000472ee2cc4d9b881818d6f4fa2afe",
      "6ec744e9208c47038d7783665d2f2541",
      "e1c008cc0a8a4b9b935dd4f3af434a56",
      "7aea357461de4f9baa8cd7bcee225f13",
      "02e35986fac8430aa782500222570b2c"
     ]
    },
    "id": "1vpGEow6yqmo",
    "outputId": "363f7923-033e-4a8d-f3f6-81ac8c582ab9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at savasy/bert-base-turkish-ner-cased were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b457ecb846346e4a59898d251c4e6f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
      "INFO:stanza:Downloading default packages for language: tr (Turkish) ...\n",
      "INFO:stanza:File exists: /root/stanza_resources/tr/default.zip\n",
      "INFO:stanza:Finished downloading models and saved to /root/stanza_resources\n",
      "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749d2545fa1a4a37891bfef7ec1a1469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
      "INFO:stanza:Loading these models for language: tr (Turkish):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | imst          |\n",
      "| mwt       | imst          |\n",
      "| pos       | imst_charlm   |\n",
      "| lemma     | imst_nocharlm |\n",
      "| depparse  | imst_charlm   |\n",
      "| ner       | starlang      |\n",
      "=============================\n",
      "\n",
      "INFO:stanza:Using device: cpu\n",
      "INFO:stanza:Loading: tokenize\n",
      "INFO:stanza:Loading: mwt\n",
      "INFO:stanza:Loading: pos\n",
      "INFO:stanza:Loading: lemma\n",
      "INFO:stanza:Loading: depparse\n",
      "INFO:stanza:Loading: ner\n",
      "INFO:stanza:Done loading processors!\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# NER modeli: BERTurk (savasy)\n",
    "model_name = \"savasy/bert-base-turkish-ner-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "#Zero-Shot-Classification(RoBERTa tabanlı)\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"joeddav/xlm-roberta-large-xnli\")\n",
    "\n",
    "#Ner modeli: Stanza\n",
    "stanza.download('tr')\n",
    "nlp = stanza.Pipeline('tr')\n",
    "\n",
    "#Text-Classification\n",
    "nlp1 = pipeline(\"text-classification\", model=\"fc63/toxic-classification-model\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "id": "oatVlKaByq-f"
   },
   "outputs": [],
   "source": [
    "def lemmatization(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = []\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            lemmas.append(word.lemma.lower())\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvPqlJa13BXt"
   },
   "source": [
    "Açık Bilgi Çıkarımı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "id": "6xc4BVNDyzI0"
   },
   "outputs": [],
   "source": [
    "def event_category(text):\n",
    "    labels = [\n",
    "        \"çöpler\", \"asansör\", \"su\", \"elektrik\",\"internet\",\"otopark\",\"alt yapı\",\n",
    "        \"gürültü\", \"aydınlatma\", \"ısınma\", \"trafik\", \"kargo\",\"ulaşım\", \"yol\"\n",
    "    ]\n",
    "    result = classifier(text, candidate_labels=labels)\n",
    "    return result[\"labels\"][0], result[\"scores\"][0]\n",
    "\n",
    "\n",
    "def request_category(text):\n",
    "    labels = [\n",
    "        \"Çözüm Talebi\",\n",
    "        \"Açıklama Talebi\",\n",
    "        \"Denetim Talebi\"\n",
    "    ]\n",
    "    result = classifier(text, candidate_labels=labels)\n",
    "    best = result[\"labels\"][0]\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "id": "rprF1wjwy0_3"
   },
   "outputs": [],
   "source": [
    "def extract_all_info(text):\n",
    "    info = {\n",
    "        \"Ad Soyad\": None,\n",
    "        \"Adres veya bölge bilgisi\": \"\",\n",
    "        \"Tarih aralıkları\": [],\n",
    "        \"Kurum ismi\": [],\n",
    "        \"Olay konusu\": None,\n",
    "        \"Talep\": None\n",
    "    }\n",
    "\n",
    "    text_lower = text.lower()\n",
    "    entities = ner_pipeline(text)\n",
    "\n",
    "    aylar = [\"ocak\", \"şubat\", \"mart\", \"nisan\", \"mayıs\", \"haziran\",\"temmuz\", \"ağustos\", \"eylül\", \"ekim\", \"kasım\", \"aralık\"]\n",
    "\n",
    "    # --- 1. NER tabanlı çıkarım ---\n",
    "    for ent in entities:\n",
    "        label = ent[\"entity_group\"]\n",
    "        word = ent[\"word\"].strip()\n",
    "\n",
    "        if label == \"PER\" and not info[\"Ad Soyad\"]:\n",
    "            info[\"Ad Soyad\"] = word\n",
    "\n",
    "        elif label == \"MISC\":\n",
    "            if any(ay in word.lower() for ay in aylar):\n",
    "                 info[\"Tarih aralıkları\"].append(word)\n",
    "\n",
    "        elif label == \"LOC\":\n",
    "            if word.lower() not in aylar and word not in info[\"Adres veya bölge bilgisi\"]:\n",
    "                 info[\"Adres veya bölge bilgisi\"] += word + \" \"\n",
    "\n",
    "        elif label == \"ORG\":\n",
    "            if word and word.lower() not in aylar and word not in info[\"Kurum ismi\"]:\n",
    "                info[\"Kurum ismi\"].append(word)\n",
    "\n",
    "    # --- 2. RegEx ile zaman ifadeleri ---\n",
    "    date_pattern = (\n",
    "        r\"\\b(?:bugün|dün|şimdi|az önce|geçmişte|gelecekte|\"\n",
    "        r\"hafta sonları|hafta içleri|\"\n",
    "        r\"(?:\\d+|bir|iki|üç|dört|beş|altı|yedi|sekiz|dokuz|on)?\\s*(?:gün|hafta|ay|yıl)\"\n",
    "        r\"(?:dır|dir|den beri|önce)?|\"\n",
    "        r\"(?:yaz|kış|ilkbahar|sonbahar)(?:da|de|aylarında|dönemi)?|\"\n",
    "        r\"geceleri|gündüzleri|geçen ay|geçtiğimiz ay|gelecek yıl|bu yıl|bu hafta|\"\n",
    "        r\"(?:yağmurlu|güneşli|karlı|fırtınalı|sisli)\\s*(?:günlerde|sabahlarda|akşamlarda|zamanlarda)?|\"\n",
    "        r\"\\d{1,2}\\s+(?:ocak|şubat|mart|nisan|mayıs|haziran|temmuz|ağustos|eylül|ekim|kasım|aralık)|\"\n",
    "        r\"son \\d+ gün\"\n",
    "        r\")\\b\"\n",
    "    )\n",
    "    matches = list(re.finditer(date_pattern, text_lower, re.IGNORECASE))\n",
    "\n",
    "    info[\"Tarih aralıkları\"] = []\n",
    "    if matches:\n",
    "        for m in matches:\n",
    "            info[\"Tarih aralıkları\"].append(m.group(0).strip())\n",
    "    else:\n",
    "        info[\"Tarih aralıkları\"].append(\"Genel\")\n",
    "\n",
    "    # --- 3. RegEx ile kurum İFADELERİ ---\n",
    "    lemmatized_text = lemmatization(text)\n",
    "    organisation = [\n",
    "        \"valilik\", \"kaymakamlık\", \"belediye\", \"belediye başkanlığı\", \"müdürlük\",\n",
    "        \"bakanlık\", \"üniversite\", \"fakülte\", \"kurum\", \"daire başkanlığı\",\n",
    "        \"il müdürlüğü\", \"genel müdürlük\", \"başkanlık\", \"rektörlük\",\n",
    "        \"yetkili\", \"birim\", \"ilgili birim\",  \"idare\", \"otorite\", \"ilgili\"\n",
    "    ]\n",
    "    for keyword in organisation:\n",
    "        if keyword in lemmatized_text and keyword not in info[\"Kurum ismi\"]:\n",
    "            info[\"Kurum ismi\"].append(keyword)\n",
    "\n",
    "    # --- 4. Zero-Shot olay kategorisi ---\n",
    "    try:\n",
    "        event, score = event_category(text)\n",
    "        if score > 0.3:\n",
    "            info[\"Olay konusu\"] = event\n",
    "    except:\n",
    "        info[\"Olay konusu\"] = None\n",
    "\n",
    "    # --- 5. Zero-Shot talep kategorisi ---\n",
    "    try:\n",
    "        request = request_category(text)\n",
    "        if request:\n",
    "            info[\"Talep\"] = request\n",
    "    except:\n",
    "        info[\"Talep\"] = None\n",
    "\n",
    "    info[\"Tarih aralıkları\"] = list(set([d.strip() for d in info[\"Tarih aralıkları\"] if d.strip()]))\n",
    "    info[\"Kurum ismi\"] = list(set([k.strip() for k in info[\"Kurum ismi\"] if k.strip()]))\n",
    "    info[\"Adres veya bölge bilgisi\"] = info[\"Adres veya bölge bilgisi\"].strip()\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9VkEvhO3FxH"
   },
   "source": [
    "JSON Kaydetme İşlemi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "id": "denIDa1fy3pa"
   },
   "outputs": [],
   "source": [
    "info = extract_all_info(text)\n",
    "\n",
    "with open(\"dilekce_bilgisi.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(info, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwd0mgrD3JS0"
   },
   "source": [
    "Yorumla Anlam Çıkarma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "id": "fhlYxFtry6Do"
   },
   "outputs": [],
   "source": [
    "def classify_issue(cumle):\n",
    "    kurallar = {\n",
    "        \"Tekrarlayan sorun\": [\n",
    "            r\"\\bher\\s+(akşam|sabah|gün|gündüz|gece|pazartesi|salı|çarşamba|perşembe|cuma|cumartesi|pazar|hafta içi|hafta sonu|ay)\\b\",\n",
    "            r\"\\byine\\b\",\n",
    "            r\"\\btekrar\\b\"\n",
    "        ],\n",
    "        \"Uzun süredir devam eden sorun\": [\n",
    "            r\"\\bdefalarca\\b\",\n",
    "            r\"\\bçok\\s+kez\\b\",\n",
    "            r\"\\bbirçok\\s+kez\\b\",\n",
    "            r\"\\bkaç\\s+kere\\b\"\n",
    "        ],\n",
    "        \"Eksiklik Bildirimi\": [\n",
    "            r\"\\beksik\", r\"\\byetersiz\", r\"\\bihtiyaç\",r\"\\bmevcut değil\",r\"\\bkontrol\",r\"\\bulaşmadı\",\n",
    "            r\"\\bbulunmuyor\", r\"\\bhiç yok\", r\"\\baz sayıda\",r\"\\bbakım\",r\"\\bonarım\"\n",
    "        ],\n",
    "       \"Ciddi bir endişe olan sorun\": [\n",
    "            r\"\\bkorku\", r\"\\bendişe\", r\"\\bhasta\",r\"\\bciddi\",\n",
    "            r\"\\brisk\", r\"\\brahatsız\", r\"\\btedirgin\",r\"\\btartışma\",\n",
    "            r\"\\btehlike\", r\"\\bacilen\", r\"\\bkötü\",r\"\\bgüvensiz\",\n",
    "            r\"\\bsağlık\", r\"\\bnefes alamıyor\", r\"\\bfiziksel engel\", r\"\\bzor durum\",\n",
    "            r\"\\btedavi\", r\"\\bpanik\", r\"\\büzülmek\", r\"\\bstres\", r\"\\btehdit\",\n",
    "            r\"\\bçaresiz\"\n",
    "        ]\n",
    "\n",
    "    }\n",
    "    #lemmatized_text = lemmatization(text)\n",
    "\n",
    "    anlamlar = []\n",
    "    for anlam, kaliplar in kurallar.items():\n",
    "        for kalip in kaliplar:\n",
    "            if re.search(kalip, text, re.IGNORECASE):\n",
    "                anlamlar.append(anlam)\n",
    "                break\n",
    "\n",
    "    return anlamlar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I55ligEH3NDO"
   },
   "source": [
    "Ton Analizi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "id": "xxLUqtBAy7v_"
   },
   "outputs": [],
   "source": [
    "def language_tone_analysis(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    angry_keywords = [\n",
    "        \"artık yeter\", \"kabul edilemez\", \"şikayetçiyim\", \"çok kötü\", \"rahatsız oldum\",\n",
    "        \"bıktım\", \"sinirliyim\", \"tepki\", \"şiddetli\", \"zorbalık\", \"asla kabul\", \"adaletsiz\"\n",
    "    ]\n",
    "\n",
    "    desperate_keywords = [\n",
    "        \"çaresiz\", \"yardım edin\", \"yapacak bir şey yok\", \"umudum kalmadı\",\n",
    "        \"endişeliyim\", \"umutsuz\", \"tedirginim\", \"çözüm bulunmadı\", \"çözüm yok\"\n",
    "    ]\n",
    "\n",
    "    formal_keywords = [\n",
    "    \"sayın\", \"makamına\", \"ilgili birim\", \"arz ederim\", \"gereği\",\n",
    "    \"kurumunuza\", \"değerli\", \"hususunda\", \"dilekçe\", \"resmî\"\n",
    "    ]\n",
    "\n",
    "    polite_complaint_keywords = [\n",
    "        \"rica ediyorum\", \"talep ediyorum\", \"yardımcı olur musunuz\",\n",
    "        \"gereğinin yapılmasını\", \"bekliyorum\", \"bilgi verir misiniz\", \"yardımcı olur musunuz\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    if any(keyword in text for keyword in angry_keywords):\n",
    "        return \"Öfkeli / Tepkili\"\n",
    "\n",
    "    if any(keyword in text for keyword in desperate_keywords):\n",
    "        return \"Çaresiz\"\n",
    "\n",
    "    if any(keyword in text for keyword in formal_keywords):\n",
    "        return \"Nesnel\"\n",
    "\n",
    "    if any(keyword in text for keyword in polite_complaint_keywords):\n",
    "        return \"Kibar Şikâyet\"\n",
    "\n",
    "    return \"Sakin\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3u_mN1FU3Rmm"
   },
   "source": [
    "Dil Analizi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3o8SZ_r5y9WM",
    "outputId": "c554fdfa-84dd-4369-c6d6-297078bb8efe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "def is_formal_language(text):\n",
    "    text = text.lower()\n",
    "    header_pattern = r'\\b(sayın|değerli)\\b'\n",
    "    footer_pattern = r'\\b(arz ederim|arz ediyorum|rica ediyorum|talep ederim|talep ediyorum)\\b'\n",
    "    has_header = re.search(header_pattern, text) is not None\n",
    "    has_footer = re.search(footer_pattern, text) is not None\n",
    "    return has_header and has_footer\n",
    "\n",
    "nlp1 = pipeline(\"text-classification\", model=\"fc63/toxic-classification-model\", trust_remote_code=True)\n",
    "\n",
    "def is_offensive_or_toxic(text, threshold=0.7):\n",
    "    result = nlp1(text)[0]\n",
    "    label = result[\"label\"].lower()\n",
    "    score = result[\"score\"]\n",
    "    return (label in [\"toxic\", \"label_1\"] and score >= threshold), score\n",
    "\n",
    "def language_formality_analysis(text):\n",
    "    if is_formal_language(text):\n",
    "        return \"Resmi\"\n",
    "    elif is_offensive_or_toxic(text)[0]:\n",
    "        return \"Argo\"\n",
    "    else:\n",
    "        return \"Samimi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ji7Agz4C3VJo"
   },
   "source": [
    "Yazım Kuralları"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "id": "K8FrAtuTy_gg"
   },
   "outputs": [],
   "source": [
    "def check_spelling(text):\n",
    "    error_score = 0\n",
    "\n",
    "    if not text[0].isupper():\n",
    "        error_score += 1\n",
    "\n",
    "    if not re.search(r'[.!?]$', text.strip()):\n",
    "        error_score += 1\n",
    "\n",
    "    if re.search(r'[.!?,;:]{3,}', text):\n",
    "        error_score += 1\n",
    "\n",
    "    if re.search(r'([.,:;!?])[^ \\n]', text):\n",
    "        error_score += 1\n",
    "\n",
    "    entities = ner_pipeline(text)\n",
    "    for ent in entities:\n",
    "        label = ent[\"entity_group\"]\n",
    "        word = ent[\"word\"].strip()\n",
    "        if label in [\"PER\", \"ORG\", \"LOC\"]:\n",
    "            if word and word[0].isalpha() and not word[0].isupper():\n",
    "                error_score += 1\n",
    "\n",
    "    sentences = re.split(r'(?<=[.!?:])\\s+', text.strip())\n",
    "    for sentence in sentences[1:]:\n",
    "        if sentence and not sentence[0].isupper():\n",
    "            error_score += 1\n",
    "\n",
    "    if error_score <= 1:\n",
    "        return \"Yüksek\"\n",
    "    elif error_score <= 3:\n",
    "        return \"Orta\"\n",
    "    else:\n",
    "        return \"Düşük\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "id": "IabNxjN-zBoe"
   },
   "outputs": [],
   "source": [
    "def show_result(text):\n",
    "    info = extract_all_info(text)\n",
    "    meaning_extraction = classify_issue(text)\n",
    "    tone_analysis = language_tone_analysis(text)\n",
    "    language_formality = language_formality_analysis(text)\n",
    "    spelling_check = check_spelling(text)\n",
    "\n",
    "    print(\"\\n--- DİLEKÇE ANALİZİ ---\")\n",
    "    print(f\"Ad Soyad                : {info.get('Ad Soyad')}\")\n",
    "    print(f\"Adres                   : {info.get('Adres veya bölge bilgisi')}\")\n",
    "    print(f\"Zaman                   : {', '.join(info.get('Tarih aralıkları', []))}\")\n",
    "    print(f\"Kurum                   : {', '.join(info.get('Kurum ismi', []))}\")\n",
    "    print(f\"Olay Konusu             : {info.get('Olay konusu')}\")\n",
    "    print(f\"Talep                   : {info.get('Talep')}\")\n",
    "    print(f\"Anlam Çıkarımı          : {meaning_extraction}\")\n",
    "    print(f\"Ton                     : {tone_analysis}\")\n",
    "    print(f\"Dilin Resmiyeti         : {language_formality}\")\n",
    "    print(f\"Yazım Kurallarına Uyum  : {spelling_check}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyIkUs2s3atb"
   },
   "source": [
    "Sonuç Gösterimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gjE4wfAfzEFI",
    "outputId": "d44bcf86-05bf-47f8-dce8-3be4860a8108"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DİLEKÇE ANALİZİ ---\n",
      "Ad Soyad                : Zehra Altın\n",
      "Adres                   : Tokat\n",
      "Zaman                   : geçtiğimiz ay\n",
      "Kurum                   : valilik\n",
      "Olay Konusu             : trafik\n",
      "Talep                   : Çözüm Talebi\n",
      "Anlam Çıkarımı          : ['Ciddi bir endişe olan sorun']\n",
      "Ton                     : Nesnel\n",
      "Dilin Resmiyeti         : Resmi\n",
      "Yazım Kurallarına Uyum  : Yüksek\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(show_result(text))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
