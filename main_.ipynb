{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZSmmnL3exzRL"
   },
   "outputs": [],
   "source": [
    "!pip install pdfplumber\n",
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1BAuvuGyM7C"
   },
   "source": [
    "Kütüphaneler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "id": "8P6riIZ9yFeZ"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "import re\n",
    "import pdfplumber\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5WncIUUyN1E"
   },
   "source": [
    "PDF Çevirme İşlemi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-RCaUnmFyHoy",
    "outputId": "89097047-1ea7-4d0f-847d-6a2cf55a2de6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
      "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sayın İlgili, 1 Haziran'da gönderdiğim kargo hâlâ elime ulaşmadı. Kargo takip sisteminde sürekli \"yolda\" ifadesi yer almakta. Müşteri hizmetlerinden de tatmin edici bir yanıt alamadım. Mağduriyetimin giderilmesini ve açıklama yapılmasını talep ediyorum. Caner Arslan, Ortahisar / Trabzon\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\n",
    "with pdfplumber.open(\"/content/drive/MyDrive/case9.pdf\") as pdf:\n",
    "    for page in pdf.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text.replace('\\n', ' ')\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9o_-TwgxzNdE",
    "outputId": "4263cd90-1d26-4432-9835-b9f85d51c0ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbMHF6QayrSe"
   },
   "source": [
    "\n",
    "Modeller\n",
    "1.  BERTurk (savasy): Bu model kişi adı soyadı, adres, tarih ve kurum gibi bilgi çıkarımı için kullanılmıştır.\n",
    "2.  Stanza (Stanford): Lemmatizationişlemi için kullanılmıştır.\n",
    "3.  Zero-Shot-Classification: Olay konusu ve talep için kullanılmıştır.\n",
    "4. Text-Classification: Dilin Resmiyetininde argo kelimeleri tespit etmek için kullanılmıştır.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708,
     "referenced_widgets": [
      "aca4941545ea495499ee82603132d51c",
      "64512829f5594836a2872fd8d9a5c46c",
      "0c445cb97b3f4673a3f5b1f5bdd17d9e",
      "ad52ff05866446ccbb272199dd14d123",
      "9027463418f94264844559d637ed3be6",
      "0735ce1dd7a5421f8be19a1ed1107c2b",
      "c21c84ac57214dd19de2f0dea985a0ee",
      "d40aa0096c354ab3b7455d33ed56de51",
      "951543294f194139bf7a97e8f3bf4c96",
      "d87127613a3841d3b13d46d79bbb4cc6",
      "cc747517398a4436b729301d59166756",
      "6ee22bec5fb744f78f00d62e01702261",
      "6cc2e685830743f8b389a508f034245f",
      "7fb028216a5847649f28a723825fc387",
      "7183cc48c9544f3695fc9b1f6d67cf8f",
      "f7dbe36a77d9433f8fe79720f5660b6f",
      "325533d1c8ed4a878420dc1a0645473c",
      "bb2b19364208445988612ed020d6d33f",
      "86e3e69f2a8b4860976bb9e1669c1b54",
      "6aaf52faca914cdf8f0cc58da0e84481",
      "1f60f14ee66c4f69a61a190449707cb5",
      "e2e53697b260444d9f70a2465ab312ad"
     ]
    },
    "id": "1vpGEow6yqmo",
    "outputId": "e7a8e91d-66f5-424f-b1dd-56e99a1a4f1e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at savasy/bert-base-turkish-ner-cased were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca4941545ea495499ee82603132d51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
      "INFO:stanza:Downloading default packages for language: tr (Turkish) ...\n",
      "INFO:stanza:File exists: /root/stanza_resources/tr/default.zip\n",
      "INFO:stanza:Finished downloading models and saved to /root/stanza_resources\n",
      "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ee22bec5fb744f78f00d62e01702261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
      "INFO:stanza:Loading these models for language: tr (Turkish):\n",
      "=============================\n",
      "| Processor | Package       |\n",
      "-----------------------------\n",
      "| tokenize  | imst          |\n",
      "| mwt       | imst          |\n",
      "| pos       | imst_charlm   |\n",
      "| lemma     | imst_nocharlm |\n",
      "| depparse  | imst_charlm   |\n",
      "| ner       | starlang      |\n",
      "=============================\n",
      "\n",
      "INFO:stanza:Using device: cpu\n",
      "INFO:stanza:Loading: tokenize\n",
      "INFO:stanza:Loading: mwt\n",
      "INFO:stanza:Loading: pos\n",
      "INFO:stanza:Loading: lemma\n",
      "INFO:stanza:Loading: depparse\n",
      "INFO:stanza:Loading: ner\n",
      "INFO:stanza:Done loading processors!\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# NER modeli: BERTurk (savasy)\n",
    "model_name = \"savasy/bert-base-turkish-ner-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "#Zero-Shot-Classification(RoBERTa tabanlı)\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"joeddav/xlm-roberta-large-xnli\")\n",
    "\n",
    "#Ner modeli: Stanza\n",
    "stanza.download('tr')\n",
    "nlp = stanza.Pipeline('tr')\n",
    "\n",
    "#Text-Classification\n",
    "nlp1 = pipeline(\"text-classification\", model=\"fc63/toxic-classification-model\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oatVlKaByq-f"
   },
   "source": [
    "def lemmatization(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = []\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            lemmas.append(word.lemma.lower())\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Açık Bilgi Çıkarımı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "id": "6xc4BVNDyzI0"
   },
   "outputs": [],
   "source": [
    "def event_category(text):\n",
    "    labels = [\n",
    "        \"çöpler\", \"asansör\", \"su\", \"elektrik\",\"internet\",\"otopark\",\"alt yapı\",\n",
    "        \"gürültü\", \"aydınlatma\", \"ısınma\", \"trafik\", \"kargo\",\"ulaşım\", \"yol\"\n",
    "    ]\n",
    "    result = classifier(text, candidate_labels=labels)\n",
    "    return result[\"labels\"][0], result[\"scores\"][0]\n",
    "\n",
    "\n",
    "def request_category(text):\n",
    "    labels = [\n",
    "        \"Çözüm Talebi\",\n",
    "        \"Açıklama Talebi\",\n",
    "        \"Denetim Talebi\"\n",
    "    ]\n",
    "    result = classifier(text, candidate_labels=labels)\n",
    "    best = result[\"labels\"][0]\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rprF1wjwy0_3"
   },
   "outputs": [],
   "source": [
    "def extract_all_info(text):\n",
    "    info = {\n",
    "        \"Ad Soyad\": None,\n",
    "        \"Adres veya bölge bilgisi\": \"\",\n",
    "        \"Tarih aralıkları\": [],\n",
    "        \"Kurum ismi\": [],\n",
    "        \"Olay konusu\": None,\n",
    "        \"Talep\": None\n",
    "    }\n",
    "\n",
    "    text_lower = text.lower()\n",
    "    entities = ner_pipeline(text)\n",
    "\n",
    "    aylar = [\"ocak\", \"şubat\", \"mart\", \"nisan\", \"mayıs\", \"haziran\",\"temmuz\", \"ağustos\", \"eylül\", \"ekim\", \"kasım\", \"aralık\"]\n",
    "\n",
    "    # 1. NER tabanlı çıkarım \n",
    "    for ent in entities:\n",
    "        label = ent[\"entity_group\"]\n",
    "        word = ent[\"word\"].strip()\n",
    "\n",
    "        if label == \"PER\" and not info[\"Ad Soyad\"]:\n",
    "            info[\"Ad Soyad\"] = word\n",
    "\n",
    "        elif label == \"MISC\":\n",
    "            if any(ay in word.lower() for ay in aylar):\n",
    "                 info[\"Tarih aralıkları\"].append(word)\n",
    "\n",
    "        elif label == \"LOC\":\n",
    "            if word.lower() not in aylar and word not in info[\"Adres veya bölge bilgisi\"]:\n",
    "                 info[\"Adres veya bölge bilgisi\"] += word + \" \"\n",
    "\n",
    "        elif label == \"ORG\":\n",
    "            if word and word.lower() not in aylar and word not in info[\"Kurum ismi\"]:\n",
    "                info[\"Kurum ismi\"].append(word)\n",
    "\n",
    "    # 2. RegEx ile zaman ifadeleri\n",
    "    date_pattern = (\n",
    "        r\"\\b(?:bugün|dün|şimdi|az önce|geçmişte|gelecekte|\"\n",
    "        r\"hafta sonları|hafta içleri|\"\n",
    "        r\"(?:\\d+|bir|iki|üç|dört|beş|altı|yedi|sekiz|dokuz|on)?\\s*(?:gün|hafta|ay|yıl)\"\n",
    "        r\"(?:dır|dir|den beri|önce)?|\"\n",
    "        r\"(?:yaz|kış|ilkbahar|sonbahar)(?:da|de|aylarında|dönemi)?|\"\n",
    "        r\"geceleri|gündüzleri|geçen ay|geçtiğimiz ay|gelecek yıl|bu yıl|bu hafta|\"\n",
    "        r\"(?:yağmurlu|güneşli|karlı|fırtınalı|sisli)\\s*(?:günlerde|sabahlarda|akşamlarda|zamanlarda)?|\"\n",
    "        r\"\\d{1,2}\\s+(?:ocak|şubat|mart|nisan|mayıs|haziran|temmuz|ağustos|eylül|ekim|kasım|aralık)|\"\n",
    "        r\"son \\d+ gün\"\n",
    "        r\")\\b\"\n",
    "    )\n",
    "    matches = list(re.finditer(date_pattern, text_lower, re.IGNORECASE))\n",
    "\n",
    "    info[\"Tarih aralıkları\"] = []\n",
    "    if matches:\n",
    "        for m in matches:\n",
    "            info[\"Tarih aralıkları\"].append(m.group(0).strip())\n",
    "    else:\n",
    "        info[\"Tarih aralıkları\"].append(\"Genel\")\n",
    "\n",
    "    # 3. RegEx ile kurum İFADELERİ \n",
    "    lemmatized_text = lemmatization(text)\n",
    "    organisation = [\n",
    "        \"valilik\", \"kaymakamlık\", \"belediye\", \"belediye başkanlığı\", \"müdürlük\",\n",
    "        \"bakanlık\", \"üniversite\", \"fakülte\", \"kurum\", \"daire başkanlığı\",\n",
    "        \"il müdürlüğü\", \"genel müdürlük\", \"başkanlık\", \"rektörlük\",\n",
    "        \"yetkili\", \"birim\", \"ilgili birim\",  \"idare\", \"otorite\", \"ilgili\"\n",
    "    ]\n",
    "    for keyword in organisation:\n",
    "        if keyword in lemmatized_text and keyword not in info[\"Kurum ismi\"]:\n",
    "            info[\"Kurum ismi\"].append(keyword)\n",
    "\n",
    "    # 4. Zero-Shot olay kategorisi \n",
    "    try:\n",
    "        event, score = event_category(text)\n",
    "        if score > 0.3:\n",
    "            info[\"Olay konusu\"] = event\n",
    "    except:\n",
    "        info[\"Olay konusu\"] = None\n",
    "\n",
    "    # 5. Zero-Shot talep kategorisi\n",
    "    try:\n",
    "        request = request_category(text)\n",
    "        if request:\n",
    "            info[\"Talep\"] = request\n",
    "    except:\n",
    "        info[\"Talep\"] = None\n",
    "\n",
    "    info[\"Tarih aralıkları\"] = list(set([d.strip() for d in info[\"Tarih aralıkları\"] if d.strip()]))\n",
    "    info[\"Kurum ismi\"] = list(set([k.strip() for k in info[\"Kurum ismi\"] if k.strip()]))\n",
    "    info[\"Adres veya bölge bilgisi\"] = info[\"Adres veya bölge bilgisi\"].strip()\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON Kaydetme İşlemi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "denIDa1fy3pa"
   },
   "outputs": [],
   "source": [
    "info = extract_all_info(text)\n",
    "\n",
    "with open(\"dilekce_bilgisi.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(info, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yorumla Anlam Çıkarma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhlYxFtry6Do"
   },
   "outputs": [],
   "source": [
    "def classify_issue(cumle):\n",
    "    kurallar = {\n",
    "        \"Tekrarlayan sorun\": [\n",
    "            r\"\\bher\\s+(akşam|sabah|gün|gündüz|gece|pazartesi|salı|çarşamba|perşembe|cuma|cumartesi|pazar|hafta içi|hafta sonu|ay)\\b\",\n",
    "            r\"\\byine\\b\",\n",
    "            r\"\\btekrar\\b\"\n",
    "        ],\n",
    "        \"Uzun süredir devam eden sorun\": [\n",
    "            r\"\\bdefalarca\\b\",\n",
    "            r\"\\bçok\\s+kez\\b\",\n",
    "            r\"\\bbirçok\\s+kez\\b\",\n",
    "            r\"\\bkaç\\s+kere\\b\"\n",
    "        ],\n",
    "        \"Eksiklik Bildirimi\": [\n",
    "            r\"\\beksik\", r\"\\byetersiz\", r\"\\bihtiyaç\",r\"\\bmevcut değil\",r\"\\bkontrol\",r\"\\bulaşmadı\",\n",
    "            r\"\\bbulunmuyor\", r\"\\bhiç yok\", r\"\\baz sayıda\",r\"\\bbakım\",r\"\\bonarım\"\n",
    "        ],\n",
    "       \"Ciddi bir endişe olan sorun\": [\n",
    "            r\"\\bkorku\", r\"\\bendişe\", r\"\\bhasta\",r\"\\bciddi\",\n",
    "            r\"\\brisk\", r\"\\brahatsız\", r\"\\btedirgin\",r\"\\btartışma\",\n",
    "            r\"\\btehlike\", r\"\\bacilen\", r\"\\bkötü\",r\"\\bgüvensiz\",\n",
    "            r\"\\bsağlık\", r\"\\bnefes alamıyor\", r\"\\bfiziksel engel\", r\"\\bzor durum\",\n",
    "            r\"\\btedavi\", r\"\\bpanik\", r\"\\büzülmek\", r\"\\bstres\", r\"\\btehdit\",\n",
    "            r\"\\bçaresiz\"\n",
    "        ]\n",
    "\n",
    "    }\n",
    "\n",
    "    anlamlar = []\n",
    "    for anlam, kaliplar in kurallar.items():\n",
    "        for kalip in kaliplar:\n",
    "            if re.search(kalip, text, re.IGNORECASE):\n",
    "                anlamlar.append(anlam)\n",
    "                break\n",
    "\n",
    "    return anlamlar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ton Analizi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "id": "xxLUqtBAy7v_"
   },
   "outputs": [],
   "source": [
    "def language_tone_analysis(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    angry_keywords = [\n",
    "        \"artık yeter\", \"kabul edilemez\", \"şikayetçiyim\", \"çok kötü\", \"rahatsız oldum\",\n",
    "        \"bıktım\", \"sinirliyim\", \"tepki\", \"şiddetli\", \"zorbalık\", \"asla kabul\", \"adaletsiz\"\n",
    "    ]\n",
    "\n",
    "    desperate_keywords = [\n",
    "        \"çaresiz\", \"yardım edin\", \"yapacak bir şey yok\", \"umudum kalmadı\",\n",
    "        \"endişeliyim\", \"umutsuz\", \"tedirginim\", \"çözüm bulunmadı\", \"çözüm yok\"\n",
    "    ]\n",
    "\n",
    "    formal_keywords = [\n",
    "    \"sayın\", \"makamına\", \"ilgili birim\", \"arz ederim\", \"gereği\",\n",
    "    \"kurumunuza\", \"değerli\", \"hususunda\", \"dilekçe\", \"resmî\"\n",
    "    ]\n",
    "\n",
    "    polite_complaint_keywords = [\n",
    "        \"rica ediyorum\", \"talep ediyorum\", \"yardımcı olur musunuz\",\n",
    "        \"gereğinin yapılmasını\", \"bekliyorum\", \"bilgi verir misiniz\", \"yardımcı olur musunuz\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    if any(keyword in text for keyword in angry_keywords):\n",
    "        return \"Öfkeli / Tepkili\"\n",
    "\n",
    "    if any(keyword in text for keyword in desperate_keywords):\n",
    "        return \"Çaresiz\"\n",
    "\n",
    "    if any(keyword in text for keyword in formal_keywords):\n",
    "        return \"Nesnel\"\n",
    "\n",
    "    if any(keyword in text for keyword in polite_complaint_keywords):\n",
    "        return \"Kibar Şikâyet\"\n",
    "\n",
    "    return \"Sakin\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dil Analizi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3o8SZ_r5y9WM",
    "outputId": "a88434be-cb65-4243-a680-37ecc68e0950"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "def is_formal_language(text):\n",
    "    text = text.lower()\n",
    "    header_pattern = r'\\b(sayın|değerli)\\b'\n",
    "    footer_pattern = r'\\b(arz ederim|arz ediyorum|rica ediyorum|talep ederim|talep ediyorum)\\b'\n",
    "    has_header = re.search(header_pattern, text) is not None\n",
    "    has_footer = re.search(footer_pattern, text) is not None\n",
    "    return has_header and has_footer\n",
    "\n",
    "nlp1 = pipeline(\"text-classification\", model=\"fc63/toxic-classification-model\", trust_remote_code=True)\n",
    "\n",
    "def is_offensive_or_toxic(text, threshold=0.7):\n",
    "    result = nlp1(text)[0]\n",
    "    label = result[\"label\"].lower()\n",
    "    score = result[\"score\"]\n",
    "    return (label in [\"toxic\", \"label_1\"] and score >= threshold), score\n",
    "\n",
    "def language_formality_analysis(text):\n",
    "    if is_formal_language(text):\n",
    "        return \"Resmi\"\n",
    "    elif is_offensive_or_toxic(text)[0]:\n",
    "        return \"Argo\"\n",
    "    else:\n",
    "        return \"Samimi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yazım Kuralları"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "id": "K8FrAtuTy_gg"
   },
   "outputs": [],
   "source": [
    "def check_spelling(text):\n",
    "    error_score = 0\n",
    "\n",
    "    if not text[0].isupper():\n",
    "        error_score += 1\n",
    "\n",
    "    if not re.search(r'[.!?]$', text.strip()):\n",
    "        error_score += 1\n",
    "\n",
    "    if re.search(r'[.!?,;:]{3,}', text):\n",
    "        error_score += 1\n",
    "\n",
    "    if re.search(r'([.,:;!?])[^ \\n]', text):\n",
    "        error_score += 1\n",
    "\n",
    "    entities = ner_pipeline(text)\n",
    "    for ent in entities:\n",
    "        label = ent[\"entity_group\"]\n",
    "        word = ent[\"word\"].strip()\n",
    "        if label in [\"PER\", \"ORG\", \"LOC\"]:\n",
    "            if word and word[0].isalpha() and not word[0].isupper():\n",
    "                error_score += 1\n",
    "\n",
    "    sentences = re.split(r'(?<=[.!?:])\\s+', text.strip())\n",
    "    for sentence in sentences[1:]:\n",
    "        if sentence and not sentence[0].isupper():\n",
    "            error_score += 1\n",
    "\n",
    "    if error_score <= 1:\n",
    "        return \"Yüksek\"\n",
    "    elif error_score <= 3:\n",
    "        return \"Orta\"\n",
    "    else:\n",
    "        return \"Düşük\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sonuç Gösterimi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "id": "IabNxjN-zBoe"
   },
   "outputs": [],
   "source": [
    "def show_result(text):\n",
    "    info = extract_all_info(text)\n",
    "    meaning_extraction = classify_issue(text)\n",
    "    tone_analysis = language_tone_analysis(text)\n",
    "    language_formality = language_formality_analysis(text)\n",
    "    spelling_check = check_spelling(text)\n",
    "\n",
    "    print(\"\\n--- DİLEKÇE ANALİZİ ---\")\n",
    "    print(f\"Ad Soyad                : {info.get('Ad Soyad')}\")\n",
    "    print(f\"Adres                   : {info.get('Adres veya bölge bilgisi')}\")\n",
    "    print(f\"Zaman                   : {', '.join(info.get('Tarih aralıkları', []))}\")\n",
    "    print(f\"Kurum                   : {', '.join(info.get('Kurum ismi', []))}\")\n",
    "    print(f\"Olay Konusu             : {info.get('Olay konusu')}\")\n",
    "    print(f\"Talep                   : {info.get('Talep')}\")\n",
    "    print(f\"Anlam Çıkarımı          : {meaning_extraction}\")\n",
    "    print(f\"Ton                     : {tone_analysis}\")\n",
    "    print(f\"Dilin Resmiyeti         : {language_formality}\")\n",
    "    print(f\"Yazım Kurallarına Uyum  : {spelling_check}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gjE4wfAfzEFI",
    "outputId": "25ae8f1f-32dc-4538-d5c5-aca9e66164bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DİLEKÇE ANALİZİ ---\n",
      "Ad Soyad                : Caner Arslan\n",
      "Adres                   : Ortahisar Trabzon\n",
      "Zaman                   : 1 haziran\n",
      "Kurum                   : ilgili\n",
      "Olay Konusu             : ulaşım\n",
      "Talep                   : Açıklama Talebi\n",
      "Anlam Çıkarımı          : ['Eksiklik Bildirimi']\n",
      "Ton                     : Nesnel\n",
      "Dilin Resmiyeti         : Resmi\n",
      "Yazım Kurallarına Uyum  : Yüksek\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(show_result(text))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
